{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#LAB - 4"
      ],
      "metadata": {
        "id": "wdkN1QKYXmMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 0"
      ],
      "metadata": {
        "id": "L0Rt4haUXoRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, SimpleRNN, GRU, MultiHeadAttention, LayerNormalization, Dropout, Input, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "import random\n",
        "import re\n",
        "\n",
        "\n",
        "raw_text = \"\"\"\n",
        "artificial intelligence is transforming modern society.\n",
        "it is used in healthcare finance education and transportation.\n",
        "machine learning allows systems to improve automatically with experience.\n",
        "data plays a critical role in training intelligent systems.\n",
        "large datasets help models learn complex patterns.\n",
        "deep learning uses multi layer neural networks.\n",
        "neural networks are inspired by biological neurons.\n",
        "each neuron processes input and produces an output.\n",
        "training a neural network requires optimization techniques.\n",
        "gradient descent minimizes the loss function.\n",
        "natural language processing helps computers understand human language.\n",
        "text generation is a key task in nlp.\n",
        "language models predict the next word or character.\n",
        "recurrent neural networks handle sequential data.\n",
        "lstm and gru models address long term dependency problems.\n",
        "however rnn based models are slow for long sequences.\n",
        "transformer models changed the field of nlp.\n",
        "they rely on self attention mechanisms.\n",
        "attention allows the model to focus on relevant context.\n",
        "transformers process data in parallel.\n",
        "this makes training faster and more efficient.\n",
        "modern language models are based on transformers.\n",
        "education is being improved using artificial intelligence.\n",
        "intelligent tutoring systems personalize learning.\n",
        "automated grading saves time for teachers.\n",
        "online education platforms use recommendation systems.\n",
        "technology enhances the quality of learning experiences.\n",
        "ethical considerations are important in artificial intelligence.\n",
        "fairness transparency and accountability must be ensured.\n",
        "ai systems should be designed responsibly.\n",
        "data privacy and security are major concerns.\n",
        "researchers continue to improve ai safety.\n",
        "text generation models can create stories poems and articles.\n",
        "they are used in chatbots virtual assistants and content creation.\n",
        "generated text should be meaningful and coherent.\n",
        "evaluation of text generation is challenging.\n",
        "human judgement is often required.\n",
        "continuous learning is essential in the field of ai.\n",
        "research and innovation drive technological progress.\n",
        "students should build strong foundations in mathematics.\n",
        "programming skills are important for ai engineers.\n",
        "practical experimentation enhances understanding.\n",
        "\"\"\"\n",
        "\n",
        "# 2. Preprocessing function\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "corpus = clean_text(raw_text)\n",
        "print(f\"Corpus Length: {len(corpus)} characters\")\n",
        "print(f\"Sample: {corpus[:100]}...\")"
      ],
      "metadata": {
        "id": "glSOoGdiXpfp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3156c5e-7f5e-4cd4-a21a-514b6b5613b7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus Length: 2166 characters\n",
            "Sample: artificial intelligence is transforming modern society it is used in healthcare finance education an...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1"
      ],
      "metadata": {
        "id": "qHeaNTFLF89y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NGramModel:\n",
        "    def __init__(self, n):\n",
        "        self.n = n\n",
        "        self.ngrams = {}\n",
        "\n",
        "    def train(self, text):\n",
        "        words = text.split()\n",
        "        for i in range(len(words) - self.n):\n",
        "            seq = tuple(words[i:i + self.n]) #history\n",
        "            next_word = words[i + self.n]    #target\n",
        "\n",
        "            if seq not in self.ngrams:\n",
        "                self.ngrams[seq] = []\n",
        "            self.ngrams[seq].append(next_word)\n",
        "\n",
        "    def generate(self, seed_text, length=10):\n",
        "        words = seed_text.lower().split()\n",
        "        output = list(words)\n",
        "\n",
        "        for _ in range(length):\n",
        "            #last n words to use as context\n",
        "            context = tuple(output[-self.n:])\n",
        "\n",
        "            #if no random word then pick a random word\n",
        "            if context not in self.ngrams:\n",
        "                possible_words = list(set([val for sublist in self.ngrams.values() for val in sublist]))\n",
        "                next_word = random.choice(possible_words)\n",
        "            else:\n",
        "                next_word = random.choice(self.ngrams[context])\n",
        "\n",
        "            output.append(next_word)\n",
        "\n",
        "        return ' '.join(output)\n",
        "\n",
        "# Experiment with Bigram (N=1 history, predict next) and Trigram\n",
        "print(\"-NGram-\")\n",
        "ngram_model = NGramModel(n=1) # Using 1 previous word to predict next\n",
        "ngram_model.train(corpus)\n",
        "print(\"Generated (N-gram):\", ngram_model.generate(\"artificial\", length=15))"
      ],
      "metadata": {
        "id": "KZFqPNWCGAUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c501a4-278c-4446-c55f-b847e1f6d5e6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-NGram-\n",
            "Generated (N-gram): artificial intelligence intelligent systems should be designed responsibly data privacy and innovation drive technological progress students\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2"
      ],
      "metadata": {
        "id": "jsRRULobGA0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([corpus])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create Input Sequences\n",
        "input_sequences = []\n",
        "token_list = tokenizer.texts_to_sequences([corpus])[0]\n",
        "\n",
        "# Sliding window to create sequences\n",
        "sequence_length = 6 # Use 5 words to predict the 6th\n",
        "for i in range(1, len(token_list)):\n",
        "    n_gram_sequence = token_list[max(0, i-sequence_length):i+1]\n",
        "    input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Create Predictors and Label\n",
        "X, y = input_sequences[:,:-1], input_sequences[:,-1]\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
        "\n",
        "# Helper function to generate text for NN models\n",
        "def generate_text_nn(model, tokenizer, seed_text, max_sequence_len, next_words=10):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text\n",
        "\n",
        "# --- TASK 2: SIMPLE RNN IMPLEMENTATION ---\n",
        "print(\"\\n-RNN-\")\n",
        "model_rnn = Sequential([\n",
        "    Embedding(total_words, 64, input_length=max_sequence_len-1),\n",
        "    SimpleRNN(100), # Standard RNN Layer\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "model_rnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_rnn.fit(X, y, epochs=50, verbose=0) # Epochs reduced for speed\n",
        "\n",
        "print(\"Generated (RNN):\", generate_text_nn(model_rnn, tokenizer, \"artificial intelligence\", max_sequence_len))"
      ],
      "metadata": {
        "id": "6-ePrP4pGOG7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4728de9e-99b5-40a5-bd86-fa362d11164c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-RNN-\n",
            "Generated (RNN): artificial intelligence intelligence intelligence modern society it is is the healthcare finance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3"
      ],
      "metadata": {
        "id": "_ae9TzaDGOye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- TASK 3: LSTM & GRU IMPLEMENTATION ---\n",
        "\n",
        "# 3.1 LSTM Model\n",
        "print(\"\\n-LSTM-\")\n",
        "model_lstm = Sequential([\n",
        "    Embedding(total_words, 64, input_length=max_sequence_len-1),\n",
        "    LSTM(100), # Long Short-Term Memory Layer\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "model_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_lstm.fit(X, y, epochs=50, verbose=0)\n",
        "\n",
        "# 3.2 GRU Model\n",
        "print(\"-GRU-\")\n",
        "model_gru = Sequential([\n",
        "    Embedding(total_words, 64, input_length=max_sequence_len-1),\n",
        "    GRU(100), # Gated Recurrent Unit Layer\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "model_gru.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_gru.fit(X, y, epochs=50, verbose=0)\n",
        "\n",
        "print(\"Generated (LSTM):\", generate_text_nn(model_lstm, tokenizer, \"deep learning\", max_sequence_len))\n",
        "print(\"Generated (GRU): \", generate_text_nn(model_gru, tokenizer, \"deep learning\", max_sequence_len))"
      ],
      "metadata": {
        "id": "FjRo6E9hGVq3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af8c3a11-3cba-4706-fedc-3adb60ad62ea"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-LSTM-\n",
            "-GRU-\n",
            "Generated (LSTM): deep learning is is learning to in nlp nlp nlp the models\n",
            "Generated (GRU):  deep learning is in in intelligence intelligent systems systems help to learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4"
      ],
      "metadata": {
        "id": "_sRuJUfVGWfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, Embedding, MultiHeadAttention, LayerNormalization, Dropout, Dense, Input, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "\n",
        "# 1. Define Position Embedding Layer\n",
        "class TokenAndPositionEmbedding(Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "# 2. Define Transformer Block (FIXED: added default for training)\n",
        "class TransformerBlock(Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = Sequential([\n",
        "            Dense(ff_dim, activation=\"relu\"),\n",
        "            Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    # FIX: Set training=None as default\n",
        "    def call(self, inputs, training=None):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# 3. Build the Transformer Model\n",
        "embed_dim = 64  # Embedding size for each token\n",
        "num_heads = 2   # Number of attention heads\n",
        "ff_dim = 32     # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "# Model Architecture\n",
        "inputs = Input(shape=(max_sequence_len-1,))\n",
        "embedding_layer = TokenAndPositionEmbedding(max_sequence_len-1, total_words, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)  # Now works without passing training=...\n",
        "\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dropout(0.1)(x)\n",
        "x = Dense(20, activation=\"relu\")(x)\n",
        "outputs = Dense(total_words, activation=\"softmax\")(x)\n",
        "\n",
        "model_transformer = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "print(\"\\n-Transformer-\")\n",
        "model_transformer.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model_transformer.fit(X, y, epochs=50, verbose=0)\n",
        "\n",
        "# Generate Text Function (Reused from previous steps)\n",
        "def generate_text_nn(model, tokenizer, seed_text, max_sequence_len, next_words=10):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)[0]\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text\n",
        "\n",
        "print(\"Generated (Transformer):\", generate_text_nn(model_transformer, tokenizer, \"transformer models\", max_sequence_len))"
      ],
      "metadata": {
        "id": "0IdYS0JeGbTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a72efd23-1beb-4533-862f-f3b741c0de97"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-Transformer-\n",
            "Generated (Transformer): transformer models is is neural networks are inspired by biological neurons each\n"
          ]
        }
      ]
    }
  ]
}